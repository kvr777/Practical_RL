{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DISPLAY=:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'bash' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-15 22:16:55,920] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = np.ones((n_states,n_actions), dtype=np.int)/n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = np.random.choice(len(policy[s]),p=policy[s]) #<pick action from policy (at random with probabilities)>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #<record prev state, action and add up reward to states,actions and total_reward accordingly>\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.03 s\n",
      "mean reward = -773.31200\tthreshold = -812.0\n",
      "Wall time: 2.99 s\n",
      "mean reward = -721.68400\tthreshold = -767.0\n",
      "Wall time: 2.91 s\n",
      "mean reward = -684.90400\tthreshold = -740.0\n",
      "Wall time: 3.16 s\n",
      "mean reward = -648.61200\tthreshold = -713.0\n",
      "Wall time: 2.81 s\n",
      "mean reward = -596.34000\tthreshold = -668.0\n",
      "Wall time: 3.02 s\n",
      "mean reward = -549.66800\tthreshold = -623.0\n",
      "Wall time: 3.39 s\n",
      "mean reward = -504.42400\tthreshold = -578.0\n",
      "Wall time: 2.83 s\n",
      "mean reward = -457.49200\tthreshold = -533.0\n",
      "Wall time: 3.17 s\n",
      "mean reward = -426.24400\tthreshold = -497.0\n",
      "Wall time: 2.5 s\n",
      "mean reward = -371.18400\tthreshold = -461.0\n",
      "Wall time: 2.16 s\n",
      "mean reward = -319.05600\tthreshold = -425.0\n",
      "Wall time: 1.88 s\n",
      "mean reward = -266.73600\tthreshold = -371.0\n",
      "Wall time: 1.69 s\n",
      "mean reward = -227.90400\tthreshold = -317.0\n",
      "Wall time: 1.62 s\n",
      "mean reward = -190.36400\tthreshold = -256.3\n",
      "Wall time: 1.24 s\n",
      "mean reward = -145.30800\tthreshold = -178.0\n",
      "Wall time: 983 ms\n",
      "mean reward = -145.69600\tthreshold = -146.3\n",
      "Wall time: 844 ms\n",
      "mean reward = -100.86400\tthreshold = -111.3\n",
      "Wall time: 696 ms\n",
      "mean reward = -80.08400\tthreshold = -100.3\n",
      "Wall time: 685 ms\n",
      "mean reward = -79.35200\tthreshold = -90.3\n",
      "Wall time: 559 ms\n",
      "mean reward = -56.01600\tthreshold = -63.0\n",
      "Wall time: 526 ms\n",
      "mean reward = -42.38400\tthreshold = -51.3\n",
      "Wall time: 484 ms\n",
      "mean reward = -36.15200\tthreshold = -43.3\n",
      "Wall time: 538 ms\n",
      "mean reward = -42.16800\tthreshold = -46.0\n",
      "Wall time: 462 ms\n",
      "mean reward = -33.48000\tthreshold = -39.0\n",
      "Wall time: 498 ms\n",
      "mean reward = -34.92400\tthreshold = -37.0\n",
      "Wall time: 511 ms\n",
      "mean reward = -33.82800\tthreshold = -34.3\n",
      "Wall time: 418 ms\n",
      "mean reward = -29.13200\tthreshold = -36.0\n",
      "Wall time: 453 ms\n",
      "mean reward = -30.95600\tthreshold = -35.0\n",
      "Wall time: 419 ms\n",
      "mean reward = -27.22800\tthreshold = -29.0\n",
      "Wall time: 451 ms\n",
      "mean reward = -36.44000\tthreshold = -37.0\n",
      "Wall time: 484 ms\n",
      "mean reward = -33.88000\tthreshold = -33.6\n",
      "Wall time: 651 ms\n",
      "mean reward = -31.60400\tthreshold = -33.3\n",
      "Wall time: 973 ms\n",
      "mean reward = -23.30800\tthreshold = -26.0\n",
      "Wall time: 672 ms\n",
      "mean reward = -33.78000\tthreshold = -27.3\n",
      "Wall time: 476 ms\n",
      "mean reward = -17.28000\tthreshold = -19.0\n",
      "Wall time: 638 ms\n",
      "mean reward = -18.43200\tthreshold = -19.0\n",
      "Wall time: 407 ms\n",
      "mean reward = -31.26400\tthreshold = -22.3\n",
      "Wall time: 392 ms\n",
      "mean reward = -20.94800\tthreshold = -21.0\n",
      "Wall time: 377 ms\n",
      "mean reward = -26.03600\tthreshold = -23.0\n",
      "Wall time: 396 ms\n",
      "mean reward = -23.57200\tthreshold = -27.0\n",
      "Wall time: 391 ms\n",
      "mean reward = -25.21200\tthreshold = -26.3\n",
      "Wall time: 388 ms\n",
      "mean reward = -27.15200\tthreshold = -29.0\n",
      "Wall time: 388 ms\n",
      "mean reward = -29.33200\tthreshold = -26.0\n",
      "Wall time: 575 ms\n",
      "mean reward = -28.98000\tthreshold = -30.6\n",
      "Wall time: 552 ms\n",
      "mean reward = -24.97600\tthreshold = -28.3\n",
      "Wall time: 383 ms\n",
      "mean reward = -24.48000\tthreshold = -24.3\n",
      "Wall time: 668 ms\n",
      "mean reward = -20.44800\tthreshold = -22.3\n",
      "Wall time: 661 ms\n",
      "mean reward = -42.79600\tthreshold = -36.0\n",
      "Wall time: 572 ms\n",
      "mean reward = -40.07200\tthreshold = -33.0\n",
      "Wall time: 466 ms\n",
      "mean reward = -26.94000\tthreshold = -23.0\n",
      "Wall time: 409 ms\n",
      "mean reward = -28.69600\tthreshold = -27.3\n",
      "Wall time: 404 ms\n",
      "mean reward = -27.20000\tthreshold = -22.3\n",
      "Wall time: 471 ms\n",
      "mean reward = -28.36000\tthreshold = -27.0\n",
      "Wall time: 359 ms\n",
      "mean reward = -20.39600\tthreshold = -19.3\n",
      "Wall time: 437 ms\n",
      "mean reward = -27.44800\tthreshold = -24.3\n",
      "Wall time: 370 ms\n",
      "mean reward = -21.80400\tthreshold = -21.6\n",
      "Wall time: 404 ms\n",
      "mean reward = -29.71600\tthreshold = -24.0\n",
      "Wall time: 407 ms\n",
      "mean reward = -29.26800\tthreshold = -25.3\n",
      "Wall time: 405 ms\n",
      "mean reward = -26.69600\tthreshold = -28.0\n",
      "Wall time: 381 ms\n",
      "mean reward = -27.92400\tthreshold = -26.3\n",
      "Wall time: 371 ms\n",
      "mean reward = -22.69200\tthreshold = -19.0\n",
      "Wall time: 410 ms\n",
      "mean reward = -25.66800\tthreshold = -18.3\n",
      "Wall time: 377 ms\n",
      "mean reward = -18.48400\tthreshold = -20.0\n",
      "Wall time: 430 ms\n",
      "mean reward = -30.06400\tthreshold = -23.3\n",
      "Wall time: 397 ms\n",
      "mean reward = -28.37600\tthreshold = -25.0\n",
      "Wall time: 399 ms\n",
      "mean reward = -25.22000\tthreshold = -23.0\n",
      "Wall time: 401 ms\n",
      "mean reward = -31.54800\tthreshold = -26.0\n",
      "Wall time: 361 ms\n",
      "mean reward = -21.39600\tthreshold = -22.0\n",
      "Wall time: 355 ms\n",
      "mean reward = -20.43600\tthreshold = -17.0\n",
      "Wall time: 409 ms\n",
      "mean reward = -30.01600\tthreshold = -24.0\n",
      "Wall time: 361 ms\n",
      "mean reward = -22.00400\tthreshold = -19.0\n",
      "Wall time: 365 ms\n",
      "mean reward = -28.83600\tthreshold = -20.0\n",
      "Wall time: 360 ms\n",
      "mean reward = -21.90400\tthreshold = -20.0\n",
      "Wall time: 336 ms\n",
      "mean reward = -16.25200\tthreshold = -17.0\n",
      "Wall time: 365 ms\n",
      "mean reward = -18.02000\tthreshold = -16.0\n",
      "Wall time: 372 ms\n",
      "mean reward = -25.66400\tthreshold = -24.0\n",
      "Wall time: 409 ms\n",
      "mean reward = -26.64800\tthreshold = -21.0\n",
      "Wall time: 443 ms\n",
      "mean reward = -30.62800\tthreshold = -23.3\n",
      "Wall time: 404 ms\n",
      "mean reward = -28.03200\tthreshold = -25.0\n",
      "Wall time: 347 ms\n",
      "mean reward = -20.32400\tthreshold = -15.0\n",
      "Wall time: 357 ms\n",
      "mean reward = -15.46800\tthreshold = -15.3\n",
      "Wall time: 382 ms\n",
      "mean reward = -22.13600\tthreshold = -19.0\n",
      "Wall time: 344 ms\n",
      "mean reward = -17.48400\tthreshold = -16.0\n",
      "Wall time: 388 ms\n",
      "mean reward = -22.57200\tthreshold = -21.3\n",
      "Wall time: 430 ms\n",
      "mean reward = -33.43200\tthreshold = -25.3\n",
      "Wall time: 385 ms\n",
      "mean reward = -28.30000\tthreshold = -24.0\n",
      "Wall time: 419 ms\n",
      "mean reward = -29.21200\tthreshold = -25.3\n",
      "Wall time: 396 ms\n",
      "mean reward = -26.00000\tthreshold = -27.0\n",
      "Wall time: 384 ms\n",
      "mean reward = -25.89600\tthreshold = -25.0\n",
      "Wall time: 412 ms\n",
      "mean reward = -28.17200\tthreshold = -27.3\n",
      "Wall time: 418 ms\n",
      "mean reward = -28.67200\tthreshold = -30.3\n",
      "Wall time: 374 ms\n",
      "mean reward = -20.95600\tthreshold = -21.0\n",
      "Wall time: 358 ms\n",
      "mean reward = -21.03600\tthreshold = -17.3\n",
      "Wall time: 357 ms\n",
      "mean reward = -18.50000\tthreshold = -20.0\n",
      "Wall time: 356 ms\n",
      "mean reward = -20.46400\tthreshold = -17.0\n",
      "Wall time: 400 ms\n",
      "mean reward = -22.14800\tthreshold = -20.0\n",
      "Wall time: 377 ms\n",
      "mean reward = -22.92800\tthreshold = -23.3\n",
      "Wall time: 403 ms\n",
      "mean reward = -24.36400\tthreshold = -17.0\n",
      "Wall time: 392 ms\n",
      "mean reward = -27.05200\tthreshold = -24.0\n",
      "Wall time: 375 ms\n",
      "mean reward = -24.38800\tthreshold = -23.0\n"
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 30  #take this percent of session with highest rewards\n",
    "smoothing = 0.1  #add this thing to all counts for stability\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = [generate_session() for _ in range(n_samples)] #<generate n_samples sessions>]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "#     threshold = int(n_samples*percentile/100.) #<select percentile of your samples> \n",
    "    threshold = np.percentile(batch_rewards,percentile)\n",
    "    \n",
    "    elite_states = batch_states[batch_rewards>threshold] #<select states from sessions where rewards are above threshold> \n",
    "    elite_actions =batch_actions[batch_rewards>threshold]   #<select actions from sessions where rewards are above threshold>\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)+smoothing\n",
    "    \n",
    "    #<count all state-action occurences in elite_states and elite_actions>\n",
    "    for e in range(len(elite_states)):\n",
    "        elite_counts[elite_states[e],elite_actions[e]] +=1\n",
    "    \n",
    "\n",
    "    policy = elite_counts/elite_counts.sum(1,keepdims = True)#<normalize over each state to get probabilities>\n",
    "    \n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-15 22:17:06,893] Making new env: MountainCar-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b101e2a518>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlAEvSR1lwEtSRxnwktRR/x9nXyMR7leLrwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b100d07550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "env = gym.make(\"MountainCar-v0\").env \n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kirill.romanov\\Anaconda3\\envs\\aind\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(80,80),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = np.random.choice(n_actions,p=probs) #<sample action with such probabilities>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 40\n",
      "epoch = 0 mean reward = -9140.87750\tthreshold = -5833.8\n",
      "40 40\n",
      "epoch = 1 mean reward = -6809.66500\tthreshold = -2619.7\n",
      "40 40\n",
      "epoch = 2 mean reward = -3486.75750\tthreshold = -1505.3\n",
      "40 40\n",
      "epoch = 3 mean reward = -1991.94250\tthreshold = -1094.5\n",
      "40 40\n",
      "epoch = 4 mean reward = -1548.66000\tthreshold = -845.6\n",
      "40 40\n",
      "epoch = 5 mean reward = -1151.87000\tthreshold = -712.9\n",
      "40 40\n",
      "epoch = 6 mean reward = -864.14250\tthreshold = -559.8\n",
      "40 40\n",
      "epoch = 7 mean reward = -629.70000\tthreshold = -431.8\n",
      "39 39\n",
      "epoch = 8 mean reward = -550.13750\tthreshold = -386.0\n",
      "40 40\n",
      "epoch = 9 mean reward = -469.35250\tthreshold = -325.8\n",
      "40 40\n",
      "epoch = 10 mean reward = -407.60000\tthreshold = -285.9\n",
      "39 39\n",
      "epoch = 11 mean reward = -371.87750\tthreshold = -270.0\n",
      "39 39\n",
      "epoch = 12 mean reward = -318.47500\tthreshold = -238.0\n",
      "39 39\n",
      "epoch = 13 mean reward = -305.22500\tthreshold = -229.0\n",
      "34 34\n",
      "epoch = 14 mean reward = -282.45250\tthreshold = -200.0\n",
      "39 39\n",
      "epoch = 15 mean reward = -258.91000\tthreshold = -195.0\n",
      "39 39\n",
      "epoch = 16 mean reward = -242.38750\tthreshold = -191.0\n",
      "40 40\n",
      "epoch = 17 mean reward = -230.92750\tthreshold = -186.9\n",
      "39 39\n",
      "epoch = 18 mean reward = -216.18000\tthreshold = -168.0\n",
      "37 37\n",
      "epoch = 19 mean reward = -202.38000\tthreshold = -154.0\n",
      "39 39\n",
      "epoch = 20 mean reward = -188.80000\tthreshold = -149.0\n",
      "40 40\n",
      "epoch = 21 mean reward = -179.64250\tthreshold = -126.9\n",
      "37 37\n",
      "epoch = 22 mean reward = -166.88000\tthreshold = -122.0\n",
      "31 31\n",
      "epoch = 23 mean reward = -161.29750\tthreshold = -119.0\n",
      "37 37\n",
      "epoch = 24 mean reward = -160.22250\tthreshold = -118.0\n",
      "32 32\n",
      "epoch = 25 mean reward = -154.06000\tthreshold = -118.0\n",
      "40 40\n",
      "epoch = 26 mean reward = -153.02750\tthreshold = -115.9\n",
      "37 37\n",
      "epoch = 27 mean reward = -150.21750\tthreshold = -114.0\n",
      "29 29\n",
      "epoch = 28 mean reward = -144.68000\tthreshold = -113.0\n",
      "40 40\n",
      "epoch = 29 mean reward = -146.62500\tthreshold = -113.9\n",
      "33 33\n",
      "epoch = 30 mean reward = -143.69750\tthreshold = -113.0\n",
      "20 20\n",
      "epoch = 31 mean reward = -142.22000\tthreshold = -111.0\n",
      "23 23\n",
      "epoch = 32 mean reward = -144.73500\tthreshold = -111.0\n",
      "17 17\n",
      "epoch = 33 mean reward = -140.10250\tthreshold = -110.0\n",
      "20 20\n",
      "epoch = 34 mean reward = -144.24250\tthreshold = -110.0\n",
      "16 16\n",
      "epoch = 35 mean reward = -148.23000\tthreshold = -110.0\n",
      "32 32\n",
      "epoch = 36 mean reward = -149.61000\tthreshold = -111.0\n",
      "38 38\n",
      "epoch = 37 mean reward = -148.14750\tthreshold = -111.0\n",
      "14 14\n",
      "epoch = 38 mean reward = -146.02500\tthreshold = -110.0\n",
      "37 37\n",
      "epoch = 39 mean reward = -139.52250\tthreshold = -110.0\n",
      "27 27\n",
      "epoch = 40 mean reward = -143.72500\tthreshold = -110.0\n",
      "29 29\n",
      "epoch = 41 mean reward = -149.55000\tthreshold = -110.0\n",
      "18 18\n",
      "epoch = 42 mean reward = -146.34750\tthreshold = -110.0\n",
      "35 35\n",
      "epoch = 43 mean reward = -145.60500\tthreshold = -110.0\n",
      "14 14\n",
      "epoch = 44 mean reward = -145.53500\tthreshold = -109.0\n",
      "29 29\n",
      "epoch = 45 mean reward = -144.14750\tthreshold = -110.0\n",
      "24 24\n",
      "epoch = 46 mean reward = -138.42500\tthreshold = -109.0\n",
      "19 19\n",
      "epoch = 47 mean reward = -136.49750\tthreshold = -109.0\n",
      "18 18\n",
      "epoch = 48 mean reward = -134.54750\tthreshold = -109.0\n",
      "21 21\n",
      "epoch = 49 mean reward = -131.03250\tthreshold = -109.0\n",
      "16 16\n",
      "epoch = 50 mean reward = -132.46000\tthreshold = -109.0\n",
      "34 34\n",
      "epoch = 51 mean reward = -136.04250\tthreshold = -110.0\n",
      "23 23\n",
      "epoch = 52 mean reward = -133.52250\tthreshold = -109.0\n",
      "23 23\n",
      "epoch = 53 mean reward = -136.48000\tthreshold = -109.0\n",
      "31 31\n",
      "epoch = 54 mean reward = -133.45000\tthreshold = -109.0\n",
      "25 25\n",
      "epoch = 55 mean reward = -134.15250\tthreshold = -109.0\n",
      "18 18\n",
      "epoch = 56 mean reward = -134.94750\tthreshold = -109.0\n",
      "38 38\n",
      "epoch = 57 mean reward = -136.48750\tthreshold = -110.0\n",
      "19 19\n",
      "epoch = 58 mean reward = -136.64000\tthreshold = -109.0\n",
      "33 33\n",
      "epoch = 59 mean reward = -134.46000\tthreshold = -109.0\n",
      "35 35\n",
      "epoch = 60 mean reward = -133.50500\tthreshold = -109.0\n",
      "36 36\n",
      "epoch = 61 mean reward = -136.70500\tthreshold = -109.0\n",
      "34 34\n",
      "epoch = 62 mean reward = -135.56000\tthreshold = -109.0\n",
      "39 39\n",
      "epoch = 63 mean reward = -131.61750\tthreshold = -109.0\n",
      "29 29\n",
      "epoch = 64 mean reward = -134.11750\tthreshold = -109.0\n",
      "12 12\n",
      "epoch = 65 mean reward = -133.50000\tthreshold = -108.0\n",
      "35 35\n",
      "epoch = 66 mean reward = -134.93250\tthreshold = -109.0\n",
      "40 40\n",
      "epoch = 67 mean reward = -135.05000\tthreshold = -108.9\n",
      "34 34\n",
      "epoch = 68 mean reward = -136.67500\tthreshold = -109.0\n",
      "33 33\n",
      "epoch = 69 mean reward = -137.50250\tthreshold = -109.0\n",
      "5 5\n",
      "epoch = 70 mean reward = -134.40500\tthreshold = -108.0\n",
      "11 11\n",
      "epoch = 71 mean reward = -134.91500\tthreshold = -108.0\n",
      "26 26\n",
      "epoch = 72 mean reward = -134.05250\tthreshold = -109.0\n",
      "30 30\n",
      "epoch = 73 mean reward = -133.08250\tthreshold = -109.0\n",
      "37 37\n",
      "epoch = 74 mean reward = -131.81750\tthreshold = -109.0\n",
      "18 18\n",
      "epoch = 75 mean reward = -133.55250\tthreshold = -109.0\n",
      "33 33\n",
      "epoch = 76 mean reward = -132.82500\tthreshold = -109.0\n",
      "21 21\n",
      "epoch = 77 mean reward = -134.95250\tthreshold = -109.0\n",
      "28 28\n",
      "epoch = 78 mean reward = -131.60500\tthreshold = -109.0\n",
      "37 37\n",
      "epoch = 79 mean reward = -132.22000\tthreshold = -109.0\n",
      "40 40\n",
      "epoch = 80 mean reward = -132.50750\tthreshold = -108.9\n",
      "13 13\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,2) (200,3) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-b04b9d06a357>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#     <fit agent to predict elite_actions(y) from elite_states(X)>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melite_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melite_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kirill.romanov\\Anaconda3\\envs\\aind\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    617\u001b[0m         \"\"\"\n\u001b[1;32m--> 618\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    620\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kirill.romanov\\Anaconda3\\envs\\aind\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_STOCHASTIC_SOLVERS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m             self._fit_stochastic(X, y, activations, deltas, coef_grads,\n\u001b[1;32m--> 377\u001b[1;33m                                  intercept_grads, layer_units, incremental)\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[1;31m# Run the LBFGS solver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kirill.romanov\\Anaconda3\\envs\\aind\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit_stochastic\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads, layer_units, incremental)\u001b[0m\n\u001b[0;32m    513\u001b[0m                     batch_loss, coef_grads, intercept_grads = self._backprop(\n\u001b[0;32m    514\u001b[0m                         \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_slice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeltas\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m                         coef_grads, intercept_grads)\n\u001b[0m\u001b[0;32m    516\u001b[0m                     accumulated_loss += batch_loss * (batch_slice.stop -\n\u001b[0;32m    517\u001b[0m                                                       batch_slice.start)\n",
      "\u001b[1;32mC:\\Users\\kirill.romanov\\Anaconda3\\envs\\aind\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_backprop\u001b[1;34m(self, X, y, activations, deltas, coef_grads, intercept_grads)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mloss_func_name\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'log_loss'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_activation_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'logistic'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             \u001b[0mloss_func_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'binary_log_loss'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLOSS_FUNCTIONS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mloss_func_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[1;31m# Add L2 regularization term to loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         values = np.sum(\n",
      "\u001b[1;32mC:\\Users\\kirill.romanov\\Anaconda3\\envs\\aind\\lib\\site-packages\\sklearn\\neural_network\\_base.py\u001b[0m in \u001b[0;36mlog_loss\u001b[1;34m(y_true, y_prob)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0my_prob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (200,2) (200,3) "
     ]
    }
   ],
   "source": [
    "n_samples = 400\n",
    "percentile = 90\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(200):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = np.percentile(batch_rewards, percentile) #<select percentile of your samples>\n",
    "    \n",
    "    if np.mean(batch_rewards)>200:\n",
    "        elite_states = batch_states[batch_rewards>=threshold] #<select states from sessions where rewards are above threshold> \n",
    "        elite_actions =batch_actions[batch_rewards>=threshold]   #<select actions from sessions where rewards are above threshold>\n",
    "    else:\n",
    "        elite_states = batch_states[batch_rewards>threshold]\n",
    "        elite_actions =batch_actions[batch_rewards>threshold]\n",
    "    \n",
    "    print(len(elite_states), len(elite_actions))\n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    \n",
    "#     <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "    agent.fit(elite_states, elite_actions);\n",
    "\n",
    "\n",
    "    print(\"epoch = %.i mean reward = %.5f\\tthreshold = %.1f\"%(i, np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-15 19:57:55,912] Clearing 2 monitor files from previous run (because force=True was provided)\n",
      "[2017-04-15 19:57:55,944] Starting new video recorder writing to C:\\Users\\kirill.romanov\\Documents\\GitHub\\Practical_RL\\week1\\videos\\openaigym.video.0.11064.video000000.mp4\n",
      "[2017-04-15 19:57:56,750] Starting new video recorder writing to C:\\Users\\kirill.romanov\\Documents\\GitHub\\Practical_RL\\week1\\videos\\openaigym.video.0.11064.video000001.mp4\n",
      "[2017-04-15 19:57:57,157] Starting new video recorder writing to C:\\Users\\kirill.romanov\\Documents\\GitHub\\Practical_RL\\week1\\videos\\openaigym.video.0.11064.video000008.mp4\n",
      "[2017-04-15 19:57:57,957] Starting new video recorder writing to C:\\Users\\kirill.romanov\\Documents\\GitHub\\Practical_RL\\week1\\videos\\openaigym.video.0.11064.video000027.mp4\n",
      "[2017-04-15 19:57:58,625] Starting new video recorder writing to C:\\Users\\kirill.romanov\\Documents\\GitHub\\Practical_RL\\week1\\videos\\openaigym.video.0.11064.video000064.mp4\n",
      "[2017-04-15 19:57:59,497] Finished writing results. You can upload them to the scoreboard via gym.upload('C:\\\\Users\\\\kirill.romanov\\\\Documents\\\\GitHub\\\\Practical_RL\\\\week1\\\\videos')\n"
     ]
    }
   ],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n",
    "#run env=gym.make(\"CartPole-v0\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.11064.video000064.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part I\n",
    "\n",
    "### Tabular correntropy method\n",
    "\n",
    "You may have noticed that the taxi problem quickly converges from -10k to aroung -500 score (+- 500) and stays there. This is in part because taxi-v2 has some hard-coded randomness in the environment. Other reason is that the percentile was chosen poorly.\n",
    "\n",
    "### Tasks\n",
    "- __1.1__ (1 pt) Modify the tabular CEM (CrossEntropyMethod) code to plot distribution of rewards and threshold on each tick.\n",
    "- __1.2__ (2 pts) Find out how the algorithm performance changes if you change different percentile and different n_samples.\n",
    "\n",
    "```<YOUR ANSWER>```\n",
    "\n",
    "\n",
    "- __1.3__ (2 pts) Tune the algorithm to end up with positive average score.\n",
    "- __1.4 bonus__ (1 pt) Try to achieve a distribution where 25% or more samples score above +9.0\n",
    "- __1.5 bonus__ (2 pts) Solve and upload [Taxi-v1](https://gym.openai.com/envs/Taxi-v1) to the openai gym.\n",
    "\n",
    "It's okay to modify the existing code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework part II\n",
    "\n",
    "### Deep crossentropy method\n",
    "\n",
    "By this moment you should have got enough score on [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) to consider it solved (see the link). It's time to upload the result and get to something harder.\n",
    "\n",
    "* if you have any trouble with CartPole-v0 and feel stuck, feel free to ask us or your peers for help.\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* __2.1__ Go to the [gym site](http://gym.openai.com/), register and obtain __api key__.\n",
    "* __2.2__ (1 pt) Upload your result to gym via gym.upload (see Results tab above, the line you need is commented)\n",
    "* __2.3__ (3 pts) Pick one of environments: MountainCar-v0 or LunarLander-v2 (or both) and solve it.\n",
    "  * For MountainCar, learn to finish it in __less than 180 steps__\n",
    "  * For LunarLander, learn to get reward of __at least +50__\n",
    "  * See the tips section below, it's kinda important.\n",
    "  \n",
    "  \n",
    "* __2.4__ (1+ pt) Devise a way to speed up training at least 2x against the default version\n",
    "  * Obvious improvement: use [joblib](https://www.google.com/search?client=ubuntu&channel=fs&q=joblib&ie=utf-8&oe=utf-8)\n",
    "  * Try re-using samples from 3-5 last iterations when computing threshold and training\n",
    "  * Experiment with amount of training iterations and learning rate of the neural network (see params)\n",
    "  \n",
    "  \n",
    "### Tips\n",
    "* Gym page: [mountaincar](https://gym.openai.com/envs/MountainCar-v0), [lunarlander](https://gym.openai.com/envs/LunarLander-v2)\n",
    "* Sessions for MountainCar may last for 10k+ ticks. Make sure ```t_max``` param is at least 10k.\n",
    " * Also it may be a good idea to cut rewards via \">\" and not \">=\". If 90% of your sessions get reward of -10k and 20% are better, than if you use percentile 20% as threshold, R >= threshold __fails cut off bad sessions__ whule R > threshold works alright.\n",
    "* _issue with gym_: Some versions of gym limit game time by 200 ticks. This will prevent cem training in most cases. Make sure your agent is able to play for the specified __t_max__, and if it isn't, try `env = gym.make(\"MountainCar-v0\").env` or otherwise get rid of TimeLimit wrapper.\n",
    "* If you use old _swig_ lib for LunarLander-v2, you may get an error. See this [issue](https://github.com/openai/gym/issues/100) for solution.\n",
    "* If it won't train it's a good idea to plot reward distribution and record sessions: they may give you some clue. If they don't, call course staff :)\n",
    "* 20-neuron network is probably not enough, feel free to experiment.\n",
    "* __Please upload the results to openai gym and send links to all submissions in the e-mail__\n",
    "\n",
    "### Bonus tasks\n",
    "\n",
    "* __2.5 bonus__ Try to find a network architecture and training params that solve __both__ environments above (_Points depend on implementation_)\n",
    "\n",
    "* __2.6 bonus__ Solve continuous action space task with `MLPRegressor` or similar.\n",
    "  * [MountainCarContinuous-v0](https://gym.openai.com/envs/MountainCarContinuous-v0), [LunarLanderContinuous-v2](https://gym.openai.com/envs/LunarLanderContinuous-v2) (4+ points if it works)\n",
    "  \n",
    "* __2.7 bonus__ Use any deep learning framework of your choice to implement policy-gradient (see lectures) on any of those envs (4 +1 per env):\n",
    "  * CartPole-v0\n",
    "  * MountainCar-v0\n",
    "  * LunarLander-v2\n",
    "  * See __tips on policy gradient__ below.\n",
    "  \n",
    "\n",
    "* __2.8 bonus__ take your favorite deep learning framework and try to get above random in [Atari Breakout](https://gym.openai.com/envs/Breakout-v0) with crossentropy method over a convolutional network.\n",
    "  * Expect at least +10 points if you get this up and running, no deadlines apply ! \n",
    "  * __See tips below on where to start, they're cruicially important__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips on policy gradient\n",
    "\n",
    "* The loss function is very similar to crossentropy method. You can get away with using rewards as  __sample_weights__.\n",
    "* If your algorithm converges to a poor strategy, try regularizing with entropy or just somehow prevent agent from picking actions deterministically (e.g. when probs = 0,0,1,0,0)\n",
    "* We will use `lasagne` later in the course so you can try to [learn it](http://lasagne.readthedocs.io/en/latest/user/tutorial.html).\n",
    "* If you don't want to mess with theano just yet, try [keras](https://keras.io/getting-started/sequential-model-guide/) or [mxnet](http://mxnet.io/tutorials/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Tips on atari breakout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There's all the pre-processing and tuning done for you in the code below\n",
    "* Once you got it working, it's probably a good idea to pre-train with autoencoder or something\n",
    "* We use last 4 frames as observations to account for ball velocity\n",
    "* The code below requires ```pip install Image``` and ```pip install gym[atari]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from breakout import make_breakout\n",
    "\n",
    "env = make_breakout()\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get the initial state\n",
    "s = env.reset()\n",
    "print (s.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plot first observation. Only one frame\n",
    "plt.imshow(s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#next frame\n",
    "new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#after 10 frames\n",
    "for _ in range(10):\n",
    "    new_s,r,done, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.imshow(new_s.swapaxes(1,2).reshape(-1,64).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "< tons of your code here or elsewhere >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [aind]",
   "language": "python",
   "name": "Python [aind]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
